# 注意力机制

## 背景

早期在解决机器翻译这一类序列到序列(Sequence to Sequence)的问题时，通常采用的做法是利用一个编码器(Encoder)和一个解码器(Decoder)构建端到端的神经网络模型，但是基于编码解码的神经网络存在两个问题，拿机器翻译举例：

问题1：如果翻译的句子很长很复杂，比如直接一篇文章输进去，模型的计算量很大，并且模型的准确率下降严重。

问题2：在翻译时，可能在不同的语境下，同一个词具有不同的含义，但是网络对这些词向量并没有区分度，没有考虑词与词之间的相关性，导致翻译效果比较差。

同样在计算机视觉领域，如果输入的图像尺寸很大，做图像分类或者识别时，模型的性能就会下降。

针对这样的问题，注意力机制被提出。

> reference: [注意力机制(Attention Mechanism)浅谈 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/364819787)



## 注意力机制的作用

注意力机制就像人类观察物体一样，使用注意力机制去关注输入部分的重要内容，忽略不重要的。



## 注意力机制的分类

深度学习注意力机制分为三类：

 	1. Soft/Global Attention(软注意机制)
     	1. 软注意机制是对输入全盘接受，只不过有的给予的权重大，有的权重小。需要考虑全部输入，只是考虑不同部分用的精力不同。因此这种注意力机制的计算量会比较大。（全局注意，权重在0、1之间）
 	2. Hard/Local Attention(硬注意机制)
     	1. 硬注意机制不会全盘考虑输入而是直接对不相关项采取舍弃措施。这样做降低了计算量但是有可能丢失一些重要的信息。（局部注意，权重为binary，非零即一）
 	3. Self/Intra Attention（自注意力机制）
     	1. 这种注意力机制属于内部注意，每一项的权重有输入项之间的相互作用决定。



## 自注意力机制

### 作用

它想要解决的问题是网络接收的输入是很多向量，并且向量的大小也是不确定的。

### 作用步骤

1. 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
2. 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；
3. 第三步将权重和相应的键值value进行加权求和得到最后的attention。

<img src="https://pic3.zhimg.com/v2-470ee9b60602fb151b6d4abf63b49772_r.jpg" alt="img" style="zoom:150%;" />

> reference：[图像处理注意力机制Attention汇总（附代码） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/388122250)



### 问题：

所以我们怎样让注意力机制像人一样去以不同的权重注意不同区域？训练过程中我们是不是需要的事标记过的图像，然后学习标记图像以达到识别新图像的任务？

#### 以下是我对上面问题的一些理解：

假如，我需要对某一类医学图像进行分析，例如CT影像。任务是判断其是否患病以及病的种类。

1. 首先我们会对原始图像进行切割

2. 然后对每一个切割出来的区域编号，以编号建立区域与区域之间的联系。这种联系可以被用来判断每一项的权重。注意力机制就是作用在建立联系并得出权重的步骤中。
3. 在得到整张图像各个区域的权重后，我们就已经分辨出重要的部分和不重要的部分了。
4. 最后，通过进行分类任务完成对图片的分析













